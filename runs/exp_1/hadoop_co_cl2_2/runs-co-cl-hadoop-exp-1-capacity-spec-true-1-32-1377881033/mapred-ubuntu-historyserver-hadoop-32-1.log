2013-08-30 15:47:10,725 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobHistoryServer
STARTUP_MSG:   host = hadoop-32-1.novalocal/192.168.100.17
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.0.0-SNAPSHOT
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-3.0.0-SNAPSHOT/etc/hadoop:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-compress-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-json-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jets3t-0.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsch-0.1.42.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/zookeeper-3.4.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/stax-api-1.0.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-math-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/modules/*.jar
STARTUP_MSG:   build = https://github.com/lalithsuresh/hadoop-common -r 0cb423bd4868928bb06cd93f882bb0913172fd17; compiled by 'lsuresh' on 2013-04-08T15:47Z
STARTUP_MSG:   java = 1.7.0_17
************************************************************/
2013-08-30 15:47:10,747 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2013-08-30 15:47:12,325 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2013-08-30 15:47:12,828 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: JobHistory Init
2013-08-30 15:47:13,966 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-30 15:47:13,984 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 493, Expected: 1023
2013-08-30 15:47:13,984 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Explicitly setting permissions to : 1023, rwxrwxrwt
2013-08-30 15:47:14,006 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is inited.
2013-08-30 15:47:14,007 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Initializing Existing Jobs...
2013-08-30 15:47:14,026 INFO org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: CachedHistoryStorage Init
2013-08-30 15:47:14,027 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is inited.
2013-08-30 15:47:14,027 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is inited.
2013-08-30 15:47:14,027 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is inited.
2013-08-30 15:47:14,027 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService is inited.
2013-08-30 15:47:14,027 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is inited.
2013-08-30 15:47:14,117 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2013-08-30 15:47:14,240 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2013-08-30 15:47:14,240 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobHistoryServer metrics system started
2013-08-30 15:47:14,253 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-30 15:47:14,257 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is started.
2013-08-30 15:47:14,257 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is started.
2013-08-30 15:47:14,257 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2013-08-30 15:47:14,258 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-30 15:47:14,267 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is started.
2013-08-30 15:47:14,362 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2013-08-30 15:47:14,507 INFO org.apache.hadoop.http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2013-08-30 15:47:14,512 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context jobhistory
2013-08-30 15:47:14,512 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2013-08-30 15:47:14,513 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2013-08-30 15:47:14,520 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory
2013-08-30 15:47:14,521 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory/*
2013-08-30 15:47:14,521 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws
2013-08-30 15:47:14,521 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws/*
2013-08-30 15:47:14,524 INFO org.apache.hadoop.http.HttpServer: Added global filter 'guice' (class=com.google.inject.servlet.GuiceFilter)
2013-08-30 15:47:14,529 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 19888
2013-08-30 15:47:14,529 INFO org.mortbay.log: jetty-6.1.26
2013-08-30 15:47:14,587 INFO org.mortbay.log: Extract jar:file:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/jobhistory to /tmp/Jetty_0_0_0_0_19888_jobhistory____.djq1tw/webapp
2013-08-30 15:47:15,069 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:19888
2013-08-30 15:47:15,072 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /jobhistory started at 19888
2013-08-30 15:47:15,650 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2013-08-30 15:47:15,695 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 10020
2013-08-30 15:47:15,740 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB to the server
2013-08-30 15:47:15,741 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2013-08-30 15:47:15,744 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 10020: starting
2013-08-30 15:47:15,755 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryClientService: Instantiated MRClientService at hadoop-32-1.novalocal/192.168.100.17:10020
2013-08-30 15:47:15,755 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is started.
2013-08-30 15:47:15,755 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is started.
2013-08-30 15:47:44,267 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner started
2013-08-30 15:47:44,279 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner complete
2013-08-30 15:50:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 15:53:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 15:53:14,618 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0001,submitTime=1377877662871,launchTime=1377877673250,firstMapTaskLaunchTime=1377877676393,firstReduceTaskLaunchTime=0,finishTime=1377877851264,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=341,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 15:53:14,619 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0001.summary]
2013-08-30 15:53:14,644 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-30 15:53:14,644 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0001-1377877662871-ubuntu-TeraGen-1377877851264-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0001-1377877662871-ubuntu-TeraGen-1377877851264-2-0-SUCCEEDED-default.jhist
2013-08-30 15:53:14,659 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0001_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0001_conf.xml
2013-08-30 15:56:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 15:56:14,298 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0002,submitTime=1377877859199,launchTime=1377877868997,firstMapTaskLaunchTime=1377877871279,firstReduceTaskLaunchTime=0,finishTime=1377878040974,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=326,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 15:56:14,298 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0002.summary]
2013-08-30 15:56:14,309 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0002-1377877859199-ubuntu-TeraGen-1377878040974-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0002-1377877859199-ubuntu-TeraGen-1377878040974-2-0-SUCCEEDED-default.jhist
2013-08-30 15:56:14,314 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0002_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0002_conf.xml
2013-08-30 15:59:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 15:59:14,304 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0003,submitTime=1377878049820,launchTime=1377878057280,firstMapTaskLaunchTime=1377878059674,firstReduceTaskLaunchTime=0,finishTime=1377878228485,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=333,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 15:59:14,304 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0003.summary]
2013-08-30 15:59:14,313 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0003-1377878049820-ubuntu-TeraGen-1377878228485-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0003-1377878049820-ubuntu-TeraGen-1377878228485-2-0-SUCCEEDED-default.jhist
2013-08-30 15:59:14,317 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0003_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0003_conf.xml
2013-08-30 16:02:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:02:14,296 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0004,submitTime=1377878236049,launchTime=1377878246323,firstMapTaskLaunchTime=1377878248564,firstReduceTaskLaunchTime=0,finishTime=1377878414150,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=326,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 16:02:14,296 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0004.summary]
2013-08-30 16:02:14,304 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0004-1377878236049-ubuntu-TeraGen-1377878414150-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0004-1377878236049-ubuntu-TeraGen-1377878414150-2-0-SUCCEEDED-default.jhist
2013-08-30 16:02:14,309 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0004_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0004_conf.xml
2013-08-30 16:05:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:05:14,292 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0005,submitTime=1377878421896,launchTime=1377878430802,firstMapTaskLaunchTime=1377878433141,firstReduceTaskLaunchTime=0,finishTime=1377878595913,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=319,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 16:05:14,293 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0005.summary]
2013-08-30 16:05:14,301 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0005-1377878421896-ubuntu-TeraGen-1377878595913-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0005-1377878421896-ubuntu-TeraGen-1377878595913-2-0-SUCCEEDED-default.jhist
2013-08-30 16:05:14,305 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0005_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0005_conf.xml
2013-08-30 16:08:14,266 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:11:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:14:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:17:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:20:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:20:14,287 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0007,submitTime=1377878619923,launchTime=1377878624683,firstMapTaskLaunchTime=1377878627122,firstReduceTaskLaunchTime=1377878700104,finishTime=1377879595058,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=1849,reduceSlotSeconds=894,jobName=TeraSort
2013-08-30 16:20:14,287 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0007.summary]
2013-08-30 16:20:14,295 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0007-1377878619923-ubuntu-TeraSort-1377879595058-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0007-1377878619923-ubuntu-TeraSort-1377879595058-40-1-SUCCEEDED-default.jhist
2013-08-30 16:20:14,299 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0007_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0007_conf.xml
2013-08-30 16:23:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:24:14,296 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/192.168.100.33:50010]
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/192.168.100.33:50010]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:24:14,300 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:24:14,300 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 665.2320411654379 msec.
2013-08-30 16:24:17,976 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:24:17,978 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:24:17,978 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 8973.309155051309 msec.
2013-08-30 16:24:29,052 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:24:29,053 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:24:29,053 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11834.381601240279 msec.
2013-08-30 16:24:43,896 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:24:43,897 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:24:43,898 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:26:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:29:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:29:14,291 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0006,submitTime=1377878619686,launchTime=1377878627799,firstMapTaskLaunchTime=1377878646993,firstReduceTaskLaunchTime=1377878686074,finishTime=1377880075550,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=4635,reduceSlotSeconds=1389,jobName=TeraSort
2013-08-30 16:29:14,291 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0006.summary]
2013-08-30 16:29:14,292 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377877617257_0009,submitTime=1377878620634,launchTime=1377878629541,firstMapTaskLaunchTime=1377878838812,firstReduceTaskLaunchTime=1377878892332,finishTime=1377880082638,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=3212,reduceSlotSeconds=1190,jobName=TeraSort
2013-08-30 16:29:14,292 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0009.summary]
2013-08-30 16:29:14,299 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0006-1377878619686-ubuntu-TeraSort-1377880075550-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0006-1377878619686-ubuntu-TeraSort-1377880075550-40-1-SUCCEEDED-default.jhist
2013-08-30 16:29:14,301 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0009-1377878620634-ubuntu-TeraSort-1377880082638-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0009-1377878620634-ubuntu-TeraSort-1377880082638-40-1-SUCCEEDED-default.jhist
2013-08-30 16:29:14,303 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0006_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0006_conf.xml
2013-08-30 16:29:14,305 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0009_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377877617257_0009_conf.xml
2013-08-30 16:29:17,284 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:29:17,285 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:29:17,286 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1348.9731064571665 msec.
2013-08-30 16:29:20,284 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:29:20,285 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:29:20,285 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 8840.798817632567 msec.
2013-08-30 16:29:32,132 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:29:32,133 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:29:32,134 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 11898.540123954866 msec.
2013-08-30 16:29:47,040 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:29:47,041 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:29:47,042 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:32:17,280 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:17,281 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:32:17,281 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1329.0895607894752 msec.
2013-08-30 16:32:20,280 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:20,281 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:32:20,281 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 4858.782271753325 msec.
2013-08-30 16:32:28,144 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:28,145 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 16:32:28,145 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 6680.61483095834 msec.
2013-08-30 16:32:37,828 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.33:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:37,829 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:32:37,830 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1958081095-192.168.100.17-1377877593140:blk_-8041533866459435152_1565 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377877617257_0008.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 16:35:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:38:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 16:41:14,264 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
