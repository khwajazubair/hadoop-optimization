2013-08-30 17:51:44,509 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobHistoryServer
STARTUP_MSG:   host = hadoop-32-1.novalocal/192.168.100.22
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.0.0-SNAPSHOT
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-3.0.0-SNAPSHOT/etc/hadoop:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-compress-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-json-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jets3t-0.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsch-0.1.42.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/zookeeper-3.4.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/stax-api-1.0.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-math-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/modules/*.jar
STARTUP_MSG:   build = https://github.com/lalithsuresh/hadoop-common -r 0cb423bd4868928bb06cd93f882bb0913172fd17; compiled by 'lsuresh' on 2013-04-08T15:47Z
STARTUP_MSG:   java = 1.7.0_17
************************************************************/
2013-08-30 17:51:44,523 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2013-08-30 17:51:45,938 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2013-08-30 17:51:46,496 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: JobHistory Init
2013-08-30 17:51:48,059 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-30 17:51:48,079 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 493, Expected: 1023
2013-08-30 17:51:48,080 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Explicitly setting permissions to : 1023, rwxrwxrwt
2013-08-30 17:51:48,115 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is inited.
2013-08-30 17:51:48,115 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Initializing Existing Jobs...
2013-08-30 17:51:48,136 INFO org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: CachedHistoryStorage Init
2013-08-30 17:51:48,137 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is inited.
2013-08-30 17:51:48,137 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is inited.
2013-08-30 17:51:48,138 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is inited.
2013-08-30 17:51:48,138 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService is inited.
2013-08-30 17:51:48,138 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is inited.
2013-08-30 17:51:48,224 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2013-08-30 17:51:48,334 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2013-08-30 17:51:48,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobHistoryServer metrics system started
2013-08-30 17:51:48,351 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-30 17:51:48,355 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is started.
2013-08-30 17:51:48,355 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is started.
2013-08-30 17:51:48,355 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2013-08-30 17:51:48,355 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-30 17:51:48,367 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is started.
2013-08-30 17:51:48,460 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2013-08-30 17:51:48,616 INFO org.apache.hadoop.http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2013-08-30 17:51:48,621 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context jobhistory
2013-08-30 17:51:48,621 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2013-08-30 17:51:48,621 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2013-08-30 17:51:48,630 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory
2013-08-30 17:51:48,630 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory/*
2013-08-30 17:51:48,631 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws
2013-08-30 17:51:48,631 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws/*
2013-08-30 17:51:48,633 INFO org.apache.hadoop.http.HttpServer: Added global filter 'guice' (class=com.google.inject.servlet.GuiceFilter)
2013-08-30 17:51:48,639 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 19888
2013-08-30 17:51:48,639 INFO org.mortbay.log: jetty-6.1.26
2013-08-30 17:51:48,697 INFO org.mortbay.log: Extract jar:file:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/jobhistory to /tmp/Jetty_0_0_0_0_19888_jobhistory____.djq1tw/webapp
2013-08-30 17:51:49,183 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:19888
2013-08-30 17:51:49,186 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /jobhistory started at 19888
2013-08-30 17:51:50,028 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2013-08-30 17:51:50,081 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 10020
2013-08-30 17:51:50,124 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB to the server
2013-08-30 17:51:50,125 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2013-08-30 17:51:50,132 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 10020: starting
2013-08-30 17:51:50,143 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryClientService: Instantiated MRClientService at hadoop-32-1.novalocal/192.168.100.22:10020
2013-08-30 17:51:50,143 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is started.
2013-08-30 17:51:50,143 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is started.
2013-08-30 17:52:18,367 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner started
2013-08-30 17:52:18,375 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner complete
2013-08-30 17:54:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 17:57:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 17:57:48,727 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0001,submitTime=1377885136171,launchTime=1377885147193,firstMapTaskLaunchTime=1377885150317,firstReduceTaskLaunchTime=0,finishTime=1377885329159,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=353,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 17:57:48,727 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0001.summary]
2013-08-30 17:57:48,754 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-30 17:57:48,754 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0001-1377885136171-ubuntu-TeraGen-1377885329159-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0001-1377885136171-ubuntu-TeraGen-1377885329159-2-0-SUCCEEDED-default.jhist
2013-08-30 17:57:48,768 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0001_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0001_conf.xml
2013-08-30 18:00:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:00:48,398 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0002,submitTime=1377885337685,launchTime=1377885344549,firstMapTaskLaunchTime=1377885347696,firstReduceTaskLaunchTime=0,finishTime=1377885515820,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=330,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 18:00:48,398 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0002.summary]
2013-08-30 18:00:48,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0002-1377885337685-ubuntu-TeraGen-1377885515820-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0002-1377885337685-ubuntu-TeraGen-1377885515820-2-0-SUCCEEDED-default.jhist
2013-08-30 18:00:48,416 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0002_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0002_conf.xml
2013-08-30 18:03:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:03:48,403 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0003,submitTime=1377885524411,launchTime=1377885530125,firstMapTaskLaunchTime=1377885532453,firstReduceTaskLaunchTime=0,finishTime=1377885711922,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=340,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 18:03:48,404 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0003.summary]
2013-08-30 18:03:48,412 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0003-1377885524411-ubuntu-TeraGen-1377885711922-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0003-1377885524411-ubuntu-TeraGen-1377885711922-2-0-SUCCEEDED-default.jhist
2013-08-30 18:03:48,423 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0003_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0003_conf.xml
2013-08-30 18:06:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:06:48,393 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0004,submitTime=1377885721243,launchTime=1377885729096,firstMapTaskLaunchTime=1377885731401,firstReduceTaskLaunchTime=0,finishTime=1377885903506,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=341,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 18:06:48,393 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0004.summary]
2013-08-30 18:06:48,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0004-1377885721243-ubuntu-TeraGen-1377885903506-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0004-1377885721243-ubuntu-TeraGen-1377885903506-2-0-SUCCEEDED-default.jhist
2013-08-30 18:06:48,420 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0004_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0004_conf.xml
2013-08-30 18:09:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:09:48,395 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0005,submitTime=1377885912024,launchTime=1377885920603,firstMapTaskLaunchTime=1377885922864,firstReduceTaskLaunchTime=0,finishTime=1377886095319,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=343,reduceSlotSeconds=0,jobName=TeraGen
2013-08-30 18:09:48,395 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0005.summary]
2013-08-30 18:09:48,403 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0005-1377885912024-ubuntu-TeraGen-1377886095319-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0005-1377885912024-ubuntu-TeraGen-1377886095319-2-0-SUCCEEDED-default.jhist
2013-08-30 18:09:48,407 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0005_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0005_conf.xml
2013-08-30 18:12:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:15:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:18:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:21:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:24:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:24:48,398 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0007,submitTime=1377886119076,launchTime=1377886126815,firstMapTaskLaunchTime=1377886222889,firstReduceTaskLaunchTime=1377886290936,finishTime=1377886995096,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=2561,reduceSlotSeconds=704,jobName=TeraSort
2013-08-30 18:24:48,398 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0007.summary]
2013-08-30 18:24:48,406 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0007-1377886119076-ubuntu-TeraSort-1377886995096-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0007-1377886119076-ubuntu-TeraSort-1377886995096-40-1-SUCCEEDED-default.jhist
2013-08-30 18:24:48,409 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0007_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0007_conf.xml
2013-08-30 18:27:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:27:48,392 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0009,submitTime=1377886119159,launchTime=1377886125417,firstMapTaskLaunchTime=1377886427389,firstReduceTaskLaunchTime=1377886501141,finishTime=1377887259847,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=3219,reduceSlotSeconds=758,jobName=TeraSort
2013-08-30 18:27:48,392 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0009.summary]
2013-08-30 18:27:48,400 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0009-1377886119159-ubuntu-TeraSort-1377887259847-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0009-1377886119159-ubuntu-TeraSort-1377887259847-40-1-SUCCEEDED-default.jhist
2013-08-30 18:27:48,405 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0009_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0009_conf.xml
2013-08-30 18:27:52,386 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:27:52,391 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:27:52,391 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1293.646434111199 msec.
2013-08-30 18:27:56,687 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:27:56,688 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:27:56,688 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 6560.687339663877 msec.
2013-08-30 18:28:06,251 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:28:06,252 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:28:06,253 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 9968.43549669978 msec.
2013-08-30 18:28:19,227 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:28:19,229 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:28:19,230 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:30:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:30:48,394 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377885090977_0008,submitTime=1377886119157,launchTime=1377886125871,firstMapTaskLaunchTime=1377886349751,firstReduceTaskLaunchTime=1377886409576,finishTime=1377887420398,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=3786,reduceSlotSeconds=1010,jobName=TeraSort
2013-08-30 18:30:48,394 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0008.summary]
2013-08-30 18:30:48,402 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0008-1377886119157-ubuntu-TeraSort-1377887420398-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0008-1377886119157-ubuntu-TeraSort-1377887420398-40-1-SUCCEEDED-default.jhist
2013-08-30 18:30:48,407 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0008_conf.xml to hdfs://hadoop-32-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/30/000000/job_1377885090977_0008_conf.xml
2013-08-30 18:30:51,387 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:30:51,389 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:30:51,389 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 572.8625590645482 msec.
2013-08-30 18:30:54,387 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:30:54,388 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:30:54,389 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 7722.952718217223 msec.
2013-08-30 18:31:05,115 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:31:05,116 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:31:05,117 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 13704.079333186522 msec.
2013-08-30 18:31:21,831 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:31:21,832 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:31:21,833 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:33:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:33:51,387 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:33:51,389 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:33:51,389 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 677.3057924802062 msec.
2013-08-30 18:33:54,387 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:33:54,388 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:33:54,388 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3444.654672116939 msec.
2013-08-30 18:34:00,835 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:34:00,837 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-30 18:34:00,837 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 9034.496703440014 msec.
2013-08-30 18:34:12,879 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.36:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:34:12,880 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:34:12,881 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1978847193-192.168.100.22-1377885066436:blk_-4959340678668283413_1540 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377885090977_0006.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-30 18:36:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:39:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-30 18:42:48,365 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
