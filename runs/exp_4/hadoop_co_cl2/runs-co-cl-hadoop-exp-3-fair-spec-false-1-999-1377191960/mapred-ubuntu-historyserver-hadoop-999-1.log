2013-08-22 16:21:30,695 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobHistoryServer
STARTUP_MSG:   host = hadoop-999-1.novalocal/192.168.100.22
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 3.0.0-SNAPSHOT
STARTUP_MSG:   classpath = /home/ubuntu/hadoop-3.0.0-SNAPSHOT/etc/hadoop:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-compress-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-auth-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-json-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jets3t-0.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jettison-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xz-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-net-3.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/slf4j-api-1.6.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-collections-3.2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsch-0.1.42.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/zookeeper-3.4.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/stax-api-1.0.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-math-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/activation-1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT-tests.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-el-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-lang-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/commons-logging-1.1.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/hdfs/hadoop-hdfs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-api-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-client-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-site-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/avro-1.5.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/commons-io-2.1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/hadoop-annotations-3.0.0-SNAPSHOT.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-core-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-guice-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/jersey-server-1.8.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/junit-4.8.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/netty-3.5.11.Final.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/protobuf-java-2.4.0a.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/mapreduce/lib/snappy-java-1.0.3.2.jar:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/modules/*.jar
STARTUP_MSG:   build = https://github.com/lalithsuresh/hadoop-common -r 0cb423bd4868928bb06cd93f882bb0913172fd17; compiled by 'lsuresh' on 2013-04-08T15:47Z
STARTUP_MSG:   java = 1.7.0_17
************************************************************/
2013-08-22 16:21:30,710 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: registered UNIX signal handlers for [TERM, HUP, INT]
2013-08-22 16:21:32,111 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2013-08-22 16:21:32,716 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: JobHistory Init
2013-08-22 16:21:34,005 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-22 16:21:34,026 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 493, Expected: 1023
2013-08-22 16:21:34,026 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Explicitly setting permissions to : 1023, rwxrwxrwt
2013-08-22 16:21:34,072 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is inited.
2013-08-22 16:21:34,072 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Initializing Existing Jobs...
2013-08-22 16:21:34,095 INFO org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage: CachedHistoryStorage Init
2013-08-22 16:21:34,097 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is inited.
2013-08-22 16:21:34,097 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is inited.
2013-08-22 16:21:34,097 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is inited.
2013-08-22 16:21:34,097 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService is inited.
2013-08-22 16:21:34,097 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is inited.
2013-08-22 16:21:34,208 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2013-08-22 16:21:34,348 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2013-08-22 16:21:34,349 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JobHistoryServer metrics system started
2013-08-22 16:21:34,361 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-22 16:21:34,365 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager is started.
2013-08-22 16:21:34,365 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage is started.
2013-08-22 16:21:34,365 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2013-08-22 16:21:34,366 INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens
2013-08-22 16:21:34,376 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistory is started.
2013-08-22 16:21:34,472 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2013-08-22 16:21:34,618 INFO org.apache.hadoop.http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2013-08-22 16:21:34,623 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context jobhistory
2013-08-22 16:21:34,623 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2013-08-22 16:21:34,623 INFO org.apache.hadoop.http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2013-08-22 16:21:34,633 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory
2013-08-22 16:21:34,633 INFO org.apache.hadoop.http.HttpServer: adding path spec: /jobhistory/*
2013-08-22 16:21:34,633 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws
2013-08-22 16:21:34,634 INFO org.apache.hadoop.http.HttpServer: adding path spec: /ws/*
2013-08-22 16:21:34,636 INFO org.apache.hadoop.http.HttpServer: Added global filter 'guice' (class=com.google.inject.servlet.GuiceFilter)
2013-08-22 16:21:34,641 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 19888
2013-08-22 16:21:34,641 INFO org.mortbay.log: jetty-6.1.26
2013-08-22 16:21:34,703 INFO org.mortbay.log: Extract jar:file:/home/ubuntu/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/jobhistory to /tmp/Jetty_0_0_0_0_19888_jobhistory____.djq1tw/webapp
2013-08-22 16:21:35,226 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:19888
2013-08-22 16:21:35,227 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /jobhistory started at 19888
2013-08-22 16:21:36,131 INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules
2013-08-22 16:21:36,188 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 10020
2013-08-22 16:21:36,232 INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB to the server
2013-08-22 16:21:36,234 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2013-08-22 16:21:36,237 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 10020: starting
2013-08-22 16:21:36,249 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryClientService: Instantiated MRClientService at hadoop-999-1.novalocal/192.168.100.22:10020
2013-08-22 16:21:36,249 INFO org.apache.hadoop.yarn.service.AbstractService: Service:HistoryClientService is started.
2013-08-22 16:21:36,249 INFO org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer is started.
2013-08-22 16:22:04,376 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner started
2013-08-22 16:22:04,386 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: History Cleaner complete
2013-08-22 16:24:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:27:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:27:35,087 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0001,submitTime=1377188502277,launchTime=1377188512821,firstMapTaskLaunchTime=1377188515122,firstReduceTaskLaunchTime=0,finishTime=1377188676023,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=319,reduceSlotSeconds=0,jobName=TeraGen
2013-08-22 16:27:35,087 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0001.summary]
2013-08-22 16:27:35,112 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Perms after creating 504, Expected: 504
2013-08-22 16:27:35,113 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0001-1377188502277-ubuntu-TeraGen-1377188676023-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0001-1377188502277-ubuntu-TeraGen-1377188676023-2-0-SUCCEEDED-default.jhist
2013-08-22 16:27:35,127 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0001_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0001_conf.xml
2013-08-22 16:30:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:30:34,408 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0002,submitTime=1377188684679,launchTime=1377188692313,firstMapTaskLaunchTime=1377188695532,firstReduceTaskLaunchTime=0,finishTime=1377188858029,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=320,reduceSlotSeconds=0,jobName=TeraGen
2013-08-22 16:30:34,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0002.summary]
2013-08-22 16:30:34,417 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0002-1377188684679-ubuntu-TeraGen-1377188858029-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0002-1377188684679-ubuntu-TeraGen-1377188858029-2-0-SUCCEEDED-default.jhist
2013-08-22 16:30:34,425 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0002_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0002_conf.xml
2013-08-22 16:33:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:33:34,412 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0003,submitTime=1377188865595,launchTime=1377188878355,firstMapTaskLaunchTime=1377188881032,firstReduceTaskLaunchTime=0,finishTime=1377189039407,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=314,reduceSlotSeconds=0,jobName=TeraGen
2013-08-22 16:33:34,412 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0003.summary]
2013-08-22 16:33:34,425 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0003-1377188865595-ubuntu-TeraGen-1377189039407-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0003-1377188865595-ubuntu-TeraGen-1377189039407-2-0-SUCCEEDED-default.jhist
2013-08-22 16:33:34,433 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0003_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0003_conf.xml
2013-08-22 16:36:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:36:34,400 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0004,submitTime=1377189047103,launchTime=1377189056844,firstMapTaskLaunchTime=1377189059141,firstReduceTaskLaunchTime=0,finishTime=1377189215752,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=300,reduceSlotSeconds=0,jobName=TeraGen
2013-08-22 16:36:34,400 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0004.summary]
2013-08-22 16:36:34,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0004-1377189047103-ubuntu-TeraGen-1377189215752-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0004-1377189047103-ubuntu-TeraGen-1377189215752-2-0-SUCCEEDED-default.jhist
2013-08-22 16:36:34,424 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0004_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0004_conf.xml
2013-08-22 16:39:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:39:34,435 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0005,submitTime=1377189224443,launchTime=1377189235253,firstMapTaskLaunchTime=1377189237639,firstReduceTaskLaunchTime=0,finishTime=1377189395002,resourcesPerMap=1024,resourcesPerReduce=0,numMaps=2,numReduces=0,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=305,reduceSlotSeconds=0,jobName=TeraGen
2013-08-22 16:39:34,435 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0005.summary]
2013-08-22 16:39:34,444 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0005-1377189224443-ubuntu-TeraGen-1377189395002-2-0-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0005-1377189224443-ubuntu-TeraGen-1377189395002-2-0-SUCCEEDED-default.jhist
2013-08-22 16:39:34,447 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0005_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0005_conf.xml
2013-08-22 16:42:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:45:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:48:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:51:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:51:34,397 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0007,submitTime=1377189438049,launchTime=1377189450297,firstMapTaskLaunchTime=1377189452664,firstReduceTaskLaunchTime=1377189544706,finishTime=1377190285556,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=2752,reduceSlotSeconds=740,jobName=TeraSort
2013-08-22 16:51:34,397 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0007.summary]
2013-08-22 16:51:34,404 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0007-1377189438049-ubuntu-TeraSort-1377190285556-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0007-1377189438049-ubuntu-TeraSort-1377190285556-40-1-SUCCEEDED-default.jhist
2013-08-22 16:51:34,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0007_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0007_conf.xml
2013-08-22 16:54:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:57:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 16:57:34,408 INFO org.apache.hadoop.mapreduce.jobhistory.JobSummary: jobId=job_1377188477298_0010,submitTime=1377189449287,launchTime=1377189459317,firstMapTaskLaunchTime=1377189487832,firstReduceTaskLaunchTime=1377189613498,finishTime=1377190576968,resourcesPerMap=1024,resourcesPerReduce=1024,numMaps=40,numReduces=1,user=ubuntu,queue=default,status=SUCCEEDED,mapSlotSeconds=2694,reduceSlotSeconds=963,jobName=TeraSort
2013-08-22 16:57:34,408 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Deleting JobSummary file: [hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0010.summary]
2013-08-22 16:57:34,415 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0010-1377189449287-ubuntu-TeraSort-1377190576968-40-1-SUCCEEDED-default.jhist to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0010-1377189449287-ubuntu-TeraSort-1377190576968-40-1-SUCCEEDED-default.jhist
2013-08-22 16:57:34,419 INFO org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Moving hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0010_conf.xml to hdfs://hadoop-999-1:54310/tmp/hadoop-yarn/staging/history/done/2013/08/22/000000/job_1377188477298_0010_conf.xml
2013-08-22 16:57:37,398 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 16:57:37,402 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 16:57:37,402 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1741.0196903946562 msec.
2013-08-22 16:57:40,395 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 16:57:40,396 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 16:57:40,396 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 8849.361641695778 msec.
2013-08-22 16:57:52,259 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 16:57:52,260 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 16:57:52,260 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 7204.021525586106 msec.
2013-08-22 16:58:02,471 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 16:58:02,472 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0009.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 16:58:02,473 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0009.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:00:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:00:37,395 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:00:37,396 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 17:00:37,396 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 26.238262865028172 msec.
2013-08-22 17:00:40,395 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:00:40,396 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 17:00:40,396 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 4288.829881662569 msec.
2013-08-22 17:00:47,687 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:00:47,688 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2013-08-22 17:00:47,688 WARN org.apache.hadoop.hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 13073.250222329643 msec.
2013-08-22 17:01:03,775 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /192.168.100.24:50010 for block, add to deadNodes and continue. java.net.NoRouteToHostException: No route to host
java.net.NoRouteToHostException: No route to host
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:526)
	at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:931)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:01:03,776 WARN org.apache.hadoop.hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0009.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:01:03,777 ERROR org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager: Error while trying to move a job to done
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-104927308-192.168.100.22-1377188453144:blk_2506000776579447225_1617 file=/tmp/hadoop-yarn/staging/history/done_intermediate/ubuntu/job_1377188477298_0009.summary
	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:738)
	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:448)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:649)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:693)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:522)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at java.io.DataInputStream.readUTF(DataInputStream.java:589)
	at java.io.DataInputStream.readUTF(DataInputStream.java:564)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.getJobSummary(HistoryFileManager.java:770)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.access$500(HistoryFileManager.java:74)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.moveToDone(HistoryFileManager.java:289)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.access$1600(HistoryFileManager.java:226)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$1.run(HistoryFileManager.java:662)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
2013-08-22 17:03:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:06:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:09:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:12:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:15:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
2013-08-22 17:18:34,373 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistory: Starting scan to move intermediate done files
